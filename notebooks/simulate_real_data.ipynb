{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import collections\n",
    "import pickle\n",
    "from scipy.integrate import quad\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.special import logit, expit\n",
    "from scipy.stats import norm\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(ax, xs, ys, title=None, xlabel=None, ylabel=None):\n",
    "    ax.plot(xs, ys)\n",
    "    if title:\n",
    "        ax.set_title(title)\n",
    "    if xlabel:\n",
    "        ax.set_xlabel(xlabel)\n",
    "    if ylabel:\n",
    "        ax.set_ylabel(ylabel)\n",
    "    if title:\n",
    "        ax.title.set_text(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Probability Density Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(u, v, std=.1, dist='normal'):\n",
    "    if dist == 'normal':\n",
    "        return norm.pdf(u, v, std) * (norm.cdf(1, v, std) - norm.cdf(0, v, std)) ** -1\n",
    "    elif dist == 'lipschitz':\n",
    "        if v - std < u and u < v + std:\n",
    "            return 1/(min(1, v + std) - max(0, v - std))\n",
    "        else:\n",
    "            return 0\n",
    "    elif dist == 'square':\n",
    "        return (1-(u-v)**2) * (quad(lambda u: (1-(u-v)**2), 0, 1)[0]) ** -1\n",
    "        \n",
    "xs = np.linspace(0, 1, 50)\n",
    "vs = [0, .25, .5]\n",
    "full_ys = [[] for _ in range(len(vs))]\n",
    "for i, v in enumerate(vs):\n",
    "    full_ys[i] = [f(x, v) for x in xs]\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(15,5))\n",
    "for i, v in enumerate(vs):\n",
    "    plot(axs[i], xs, full_ys[i], title='v={}'.format(v), xlabel='u', ylabel='pdf_e(v,u)')\n",
    "plt.subplots_adjust(wspace=0.17, hspace=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that f is a valid PDF - these should all be approximately equal to 1\n",
    "zero_to_one = True\n",
    "for v in [0, .2, .5, .75, 1]:\n",
    "    if zero_to_one:\n",
    "        print(quad(lambda u: f(u,v), 0, 1))\n",
    "    else:\n",
    "        print(quad(lambda u: f(u,v), -20, 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute exact preimage sizes with nested integrals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high = 1 # max support of the pdf\n",
    "\n",
    "def pdf(u, v, dist='normal', std=.5):\n",
    "    # return the value of the pdf of nbhd(v) at u\n",
    "    if dist == 'uniform':\n",
    "        # uniform distribution on [0,1]\n",
    "        return 1\n",
    "    elif dist == 'lipschitz':\n",
    "        # uniform on [v-std, v+std]\n",
    "        if v - std <= u and u <= v + std:\n",
    "            return 1/(min(1, v+std)-max(0, v-std))\n",
    "        else: \n",
    "            return 0\n",
    "    elif dist == 'normal':\n",
    "        # normal dist with mean=v, std=std, scaled to be in [0,1]\n",
    "        return norm.pdf(u, v, std) * (norm.cdf(1, v, std) - norm.cdf(0, v, std)) ** -1\n",
    "\n",
    "def e1(v, s, std=.1):\n",
    "    # return the expected size of the preimage of v\n",
    "    return s * quad(lambda u: pdf(u,v,std=std), v, high)[0] \\\n",
    "            * (quad(lambda w: pdf(w,v,std=std), v, high)[0]) ** (s-1)\n",
    "\n",
    "def ex(v, s, std=.1, n=1):\n",
    "    # return the expected size of the n'th preimage of v\n",
    "    if n == 1:\n",
    "        return e1(v, s, std=std)\n",
    "    return e1(v, s, std) * (quad(lambda u: pdf(u,v, std=std), v, high)[0]) ** (-1) \\\n",
    "                    * quad(lambda u: pdf(u,v, std=std) * ex(u, s, std, n-1), v, high)[0]\n",
    "\n",
    "def full_preimage(v, s, std=.1, n=5):\n",
    "    # return the expected size of the full preimage up to n\n",
    "    if not n:\n",
    "        return 0\n",
    "    return ex(v, s, std, n) + full_preimage(v, s, std, n-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compute preimage sizes by running simulations\n",
    " - the methods in the previous cell can become slow after 3 or 4 nested integrals\n",
    " - the methods in the next cell give the same results as long as we run enough trials\n",
    " - specifically, e1, ex, and full_preimage will give the same output as s1, sx, and sample_preimage (averaged over trials)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(v, std=.3, dist='normal'):\n",
    "    # sample a random point from the nbhd of v\n",
    "    if dist == 'uniform':\n",
    "        return np.random.rand()\n",
    "    elif dist == 'lipschitz':\n",
    "        return np.random.uniform(max(0, v-std), min(1, v+std))\n",
    "    elif dist == 'normal':\n",
    "        # rejection sampling\n",
    "        u = np.random.rand()\n",
    "        y = np.random.rand() * pdf(v, v, dist='normal', std=std)\n",
    "        if y < pdf(u, v, dist='normal', std=std):\n",
    "            return u\n",
    "        else:\n",
    "            return sample(v, std=std, dist='lipschitz')\n",
    "    \n",
    "def get_nbhd(v, s, std=.3):\n",
    "    # sample a nbhd for v\n",
    "    return [sample(v, std) for _ in range(s)]\n",
    "\n",
    "def check_if_chosen(v, u, s, std=.3):\n",
    "    # check if f(u)=v by sampling a nbhd for u\n",
    "    return (v < u and v < min(get_nbhd(u, s-1, std)))\n",
    "\n",
    "def check_if_local_min(v, s, std=.3):\n",
    "    return all([v < u for u in get_nbhd(v, s, std)])\n",
    "\n",
    "def s1(v, s, std=.3):\n",
    "    # return the size of the preimage for v\n",
    "    return sum([check_if_chosen(v, u, s, std) for u in get_nbhd(v, s, std)])\n",
    "\n",
    "def sx(v, s, std=.3, n=1):\n",
    "    # return the size of the n'th preimage of v\n",
    "    if n == 1:\n",
    "        return s1(v, s, std)\n",
    "    return sum([sx(u, s, std, n-1) for u in get_nbhd(v, s, std) \n",
    "                if check_if_chosen(v, u, s, std)])\n",
    "\n",
    "def sample_preimage(v, s, std=.3, n=1):\n",
    "    # return the size of the full preimage up to n \n",
    "    if not n:\n",
    "        return 0\n",
    "    return sx(v, s, std, n) + sample_preimage(v, s, std, n-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check that sample() is working correctly\n",
    "ys = []\n",
    "for _ in range(1000):\n",
    "    ys.append(sample(v=.25, std=.3, dist='normal'))\n",
    "plt.hist(ys, bins=20)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Fit the global distribution of the datasets in NAS-Bench-201\n",
    "- get the loss files from https://drive.google.com/drive/folders/1XdXLrwCTWPPZvXrvCeJ9r0FboMyNLOhS?usp=sharing\n",
    "- cifar10_losses.pkl, cifar100_losses.pkl. ImageNet16-120_losses.pkl\n",
    "    - each file is a size 15625 array of all validation losses in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['cifar10', 'cifar100', 'ImageNet16-120']\n",
    "xlimits = [(8,20), (25,55), (53,82)]\n",
    "n = 15625\n",
    "\n",
    "global_stds = [.18, .1, .22]\n",
    "index = 2\n",
    "\n",
    "dataset = datasets[index]\n",
    "xlimit = xlimits[index]\n",
    "length = xlimit[1] - xlimit[0]\n",
    "\n",
    "losses, _ = pickle.load(open('{}_losses.pkl'.format(dataset), 'rb'))\n",
    "pruned_losses = [(l-xlimit[0]+1e-4)/length for l in losses if l < xlimit[1]]\n",
    "weights = np.ones_like(pruned_losses) / len(pruned_losses) * (xlimit[1]-xlimit[0])\n",
    "print('fraction unpruned', len(pruned_losses)/n)\n",
    "plt.hist(pruned_losses, bins=25, density=1)\n",
    "\n",
    "xs = np.linspace(0, 1, 100)\n",
    "ys = [f(x, .25, std=global_stds[index], dist='normal') for x in xs]\n",
    "plt.plot(xs, ys, color='red', linewidth=2)\n",
    "\n",
    "plt.xlabel('validation loss')\n",
    "plt.ylabel('percent of architectures')\n",
    "plt.title('Histogram of validation losses for {}'.format(dataset))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the probability that arch will converge close to opt \n",
    "### Theorem 5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_vs(size=50,\n",
    "                threshold=.1):\n",
    "    # generate a set of v's\n",
    "    vs = [*np.linspace(0, threshold, size, endpoint=False), \n",
    "      *np.linspace(threshold, 1-threshold, size, endpoint=False),\n",
    "      *np.linspace(1-threshold, 1, size, endpoint=True)]\n",
    "    return vs\n",
    "\n",
    "def compute_local_min_probs(vs,\n",
    "                           s=24,\n",
    "                           std=.3,\n",
    "                           trials=1000,\n",
    "                           save_to_file=False):\n",
    "    # compute the prob v is a local minima\n",
    "    local_min_probs = []\n",
    "    for i, v in enumerate(vs):\n",
    "        if i % (len(vs) / 10) == 0:\n",
    "            print(i, 'of', len(vs))\n",
    "        prob = 0\n",
    "        for _ in range(trials):\n",
    "            prob += check_if_local_min(v, s=s, std=std)\n",
    "        prob /= trials\n",
    "        local_min_probs.append((v, prob))\n",
    "    \n",
    "    if save_to_file:\n",
    "        pickle.dump(local_min_probs, open('local_min_probs_{}.pkl'\\\n",
    "                                          .format(std.split('.')[-1]), 'wb'))\n",
    "    return local_min_probs\n",
    "        \n",
    "def compute_preimage_sizes(vs,\n",
    "                          s=24,\n",
    "                          std=.3,\n",
    "                          trials=10,\n",
    "                          n=3,\n",
    "                          save_to_file=False):\n",
    "    # compute the size of preimages\n",
    "    preimage_sizes = []\n",
    "    for i, v in enumerate(vs):\n",
    "        if i == len(vs)//100:\n",
    "            print(i, 'of', len(vs))\n",
    "        if i % (len(vs) / 10) == 0:\n",
    "            print(i, 'of', len(vs))\n",
    "        counts = 0\n",
    "        for _ in range(trials):\n",
    "            counts += sample_preimage(v, s=s, std=std, n=n)\n",
    "        counts /= trials\n",
    "        preimage_sizes.append((v, counts))\n",
    "    if save_to_file:\n",
    "        pickle.dump(preimage_sizes, open('preimage_sizes_{}.pkl'\\\n",
    "                                         .format(std.split('.')[-1]), 'wb'))\n",
    "    return preimage_sizes\n",
    "        \n",
    "def compute_epsilon_probs(vs,\n",
    "                         local_min_probs,\n",
    "                         preimage_sizes,\n",
    "                         s=24,\n",
    "                         global_std=.3,\n",
    "                         global_shift=.25,\n",
    "                         trials=10000,\n",
    "                         save_to_file=False):\n",
    "    # now compute the full epsilon probabilities\n",
    "    \n",
    "    contribs = []\n",
    "    for t in range(trials):\n",
    "        if t % (trials / 10) == 0:\n",
    "            print(t/trials)\n",
    "        v = sample(v=global_shift, std=global_std, dist='normal')\n",
    "        index = np.argmin([abs(v - u) for u in vs])\n",
    "        contribs.append((v, local_min_probs[index][1] * preimage_sizes[index][1]))\n",
    "        \n",
    "    return contribs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check everything is working\n",
    "vs = generate_vs(size=20, threshold=.1)\n",
    "local_min_probs = compute_local_min_probs(vs, std=.3, trials=50)\n",
    "\n",
    "print('finished local min probs')\n",
    "plt.plot(vs, [y[1] for y in local_min_probs])\n",
    "plt.xscale('log')\n",
    "plt.show()\n",
    "\n",
    "preimage_sizes = compute_preimage_sizes(vs, std=.3, trials=1, n=2)\n",
    "\n",
    "print('finished preimage sizes')\n",
    "plt.plot(vs, [y[1] for y in preimage_sizes])\n",
    "plt.xscale('log')\n",
    "plt.show()\n",
    "\n",
    "contribs = compute_epsilon_probs(vs, \n",
    "                                 local_min_probs, \n",
    "                                 preimage_sizes,\n",
    "                                 trials=1000,\n",
    "                                 global_std=.5,\n",
    "                                 global_shift=.25)\n",
    "print('finished contribs')\n",
    "\n",
    "total = sum([pair[1] for pair in contribs])\n",
    "for eps in [.005, .01, .05, .1]:\n",
    "    eps_total = sum([pair[1] for pair in contribs if pair[0] < eps])\n",
    "    print('{} fraction: {}'.format(eps, eps_total / total))\n",
    "    \n",
    "xs = np.geomspace(.01, 1, 100)\n",
    "ys = [sum([pair[1] for pair in contribs if pair[0] < x]) for x in xs]\n",
    "ys = np.array(ys) / total\n",
    "plt.plot(xs, ys)\n",
    "plt.xscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Theorem 5.1 and Lemma 5.2 to real data\n",
    "- first, run using our precomputed data files from https://drive.google.com/drive/folders/1XdXLrwCTWPPZvXrvCeJ9r0FboMyNLOhS?usp=sharing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# compare real vs. simulated data using our precomputed data\n",
    "\n",
    "datasets = ['cifar10', 'cifar100', 'Random', 'ImageNet16-120', ]\n",
    "names = ['CIFAR-10', 'CIFAR-100', 'Unif. Random', 'ImageNet16-120']\n",
    "snames = ['Thm 5.1 sim. - CIFAR-10', 'Thm 5.1 sim. - CIFAR-100', 'Thm 5.1 sim. - Unif. Random', 'Thm 5.1 sim. - ImageNet16-120']\n",
    "colors = ['#00A6FF', '#FFB900', '#FF2D00', '#5DFF00']\n",
    "scolors = ['#007FC2', '#C89100', '#B72100', '#2ED800']\n",
    "\n",
    "xs = np.geomspace(.003, .1, 100)\n",
    "\n",
    "fig, axes = plt.subplots()\n",
    "\n",
    "# compute simulation from Lemma 5.2\n",
    "def lemma_5_2(eps, s=24, n=30):\n",
    "    return 1/(s+1) * (1- (1-eps)**(s+1)) \\\n",
    "        + (1-(1-eps)**(2*s+1)) * s/(s+1) \\\n",
    "        + (1-(1-eps)**s) * sum([(1-eps)**(i*s+1) * np.prod([s/(j*s+1) \\\n",
    "                                for j in range(1, i)]) for i in range(3, n)])\n",
    "\n",
    "for i, dataset in enumerate(datasets):\n",
    "    # plot the real dataset\n",
    "    percents = pickle.load(open('{}_percents.pkl'.format(datasets[i]), 'rb'))\n",
    "    ps = [sum([1 for percent in percents if (percent < x and percent < 1)]) for x in xs]\n",
    "    ps = np.array(ps) / ps[-1]\n",
    "    plt.plot(xs, ps, label=names[i], color=colors[i])\n",
    "\n",
    "    # plot the synthetic dataset\n",
    "    if dataset != 'Random':\n",
    "        contribs = pickle.load(open('synth_{}.pkl'.format(datasets[i]), 'rb'))\n",
    "        cs = np.array([sum([pair[1] for pair in contribs if pair[0] < x]) for x in xs])\n",
    "        cs /= cs[-1]\n",
    "        plt.plot(xs, cs, label='synth_{}'.format(datasets[i]), linewidth=2.5, linestyle='dashed', color=scolors[i])\n",
    "    else:\n",
    "        fs = [lemma_5_2(x) for x in xs]\n",
    "        plt.plot(xs, fs, label='synth_Random', linewidth=2.5, linestyle='dashed', color=scolors[i])\n",
    "\n",
    "lines = axes.get_lines()\n",
    "legend1 = plt.legend([lines[w] for w in [4, 6, 0, 2]], [names[n] for n in [2,3,0,1]], loc='upper left')\n",
    "legend2 = plt.legend([lines[w] for w in [5, 7, 1, 3]], [snames[n] for n in [2,3,0,1]], loc='lower right')\n",
    "axes.add_artist(legend1)\n",
    "axes.add_artist(legend2)\n",
    "plt.xscale('log')\n",
    "#plt.legend(loc='best')\n",
    "plt.xlabel('distance to global minimum')\n",
    "plt.ylabel('percent of architectures')\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run simulations from scratch (can be compute intensive)\n",
    "\n",
    "global_params = [(.25, .18), (.25, .1), (.25, .22)]\n",
    "local_params = [350, 350, 350]\n",
    "datasets = ['cifar10', 'cifar100', 'ImageNet16-120', 'random']\n",
    "\n",
    "trial_vars = [500, 5, 3, 500000]\n",
    "xs = np.geomspace(.003, .1, 100)\n",
    "    \n",
    "# plot the dataset\n",
    "for i, dataset in enumerate(datasets):\n",
    "    percents = pickle.load(open('{}_percents.pkl'.format(datasets[i]), 'rb'))\n",
    "    ps = [sum([1 for percent in percents if (percent < x and percent < 1)]) for x in xs]\n",
    "    ps = np.array(ps) / ps[-1]\n",
    "    plt.plot(xs, ps, label=dataset)\n",
    "    \n",
    "# plot new arrays + local contrib function\n",
    "vs = generate_vs(size=50, threshold=.1)\n",
    "\n",
    "for i in range(3):\n",
    "\n",
    "    local_min_probs = compute_local_min_probs(vs, std=n, trials=trial_vars[0])\n",
    "    preimage_sizes = compute_preimage_sizes(vs, std=n, trials=trial_vars[1], n=trial_vars[2])\n",
    "    \n",
    "    contribs = compute_epsilon_probs(vs, \n",
    "                                     local_min_probs, \n",
    "                                     preimage_sizes,\n",
    "                                     trials=trial_vars[3],\n",
    "                                     global_std=global_params[i][1],\n",
    "                                     global_shift=global_params[i][0])\n",
    "\n",
    "    cs = np.array([sum([pair[1] for pair in contribs if pair[0] < x]) for x in xs])\n",
    "    cs /= cs[-1]\n",
    "    plt.plot(xs, cs, label='synth_{}'.format(datasets[i]))\n",
    "\n",
    "# add random nasbench-201 data\n",
    "\n",
    "fs = [lemma_5_2(x) for x in xs]\n",
    "plt.plot(xs, fs, label='synth_random', linewidth=2.5, alpha=0.7)\n",
    "    \n",
    "plt.xscale('log')\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('distance to global minimum')\n",
    "plt.ylabel('percent of architectures')\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate preimage sizes for nas-bench-201 random data\n",
    "- get random_preimage_sizes.pkl from https://drive.google.com/drive/folders/1XdXLrwCTWPPZvXrvCeJ9r0FboMyNLOhS?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the preimage sizes of all the local minima\n",
    "\n",
    "preimage_sizes = pickle.load(open('random_preimage_sizes.pkl', 'rb'))\n",
    "\n",
    "xs = [m[0] for m in preimage_sizes if m[0] < .2] # val losses of local minima\n",
    "ys = [m[1] for m in preimage_sizes if m[0] < .2] # preimage sizes of local minima\n",
    "\n",
    "# compute the average over a sliding window\n",
    "window=100\n",
    "avg_xs = [np.mean([xs[i:i+window]]) for i in range(len(xs)-1)]\n",
    "avg_ys = [np.mean([ys[i:i+window]]) for i in range(len(ys)-1)]\n",
    "\n",
    "\n",
    "def exact_bound(v, s=24, n=20):\n",
    "    # this is Equation A.1 from Lemma A.2 in our paper\n",
    "    return 1 + s * sum([(1-v)**(s*i) * np.prod([s/(j*s+1) for j in range(1, i)]) for i in range(1, n)])\n",
    "\n",
    "thresh = .05\n",
    "x_max = .2\n",
    "size = 10000\n",
    "vs= [*np.linspace(0, thresh, size, endpoint=False), \n",
    "      *np.linspace(thresh, x_max, size, endpoint=True)]\n",
    "\n",
    "plt.plot(vs, [exact_bound(x) for x in vs], label='Lemma 5.2', linewidth=3, color='green', alpha=0.8)\n",
    "plt.scatter(avg_xs, avg_ys, s=8.8, alpha=0.8, label='NASBench-201 with losses in U([0,1])', color='red')\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('Validation loss')\n",
    "plt.ylabel('Size of full preimage')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
